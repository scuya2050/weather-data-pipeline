{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e45bb0e3-c0b6-4123-9eb6-550367e8b46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ba8858-5008-45fb-a0c9-bafa9ab10b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import schema_of_json, current_timestamp, input_file_name, lit, col, to_json, from_json, to_timestamp, to_utc_timestamp, window\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586c4a67-d311-4f5e-9d3e-c89442738a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d956e63-36bf-41fe-8714-2a5fa07286fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_catalog_name = \"weather\"\n",
    "silver_schema_name = \"02_silver\"\n",
    "silver_table_name = \"weather_sensor_measurements\"\n",
    "\n",
    "gold_catalog_name = \"weather\"\n",
    "gold_schema_name = \"03_gold\"\n",
    "gold_table_name = \"weather_sensor_statistics_per_hour\"\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {gold_catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {gold_catalog_name}.{gold_schema_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {gold_catalog_name}.{gold_schema_name}.checkpoints\")\n",
    "checkpoint_dir = f\"/Volumes/{gold_catalog_name}/{gold_schema_name}/checkpoints/{gold_table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d980c2e-ed8d-4090-9972-27974c93b3c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"delta\") \\\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\") \\\n",
    "    .table(f\"{silver_catalog_name}.{silver_schema_name}.{silver_table_name}\")\n",
    "\n",
    "aggregation_df = source_df \\\n",
    "    .withWatermark(\"measurement_timestamp\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(\"measurement_timestamp\", \"1 hour\"),\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    ) \\\n",
    "    .agg(\n",
    "        F.min(\"humidity\").alias(\"min_humidity\"),\n",
    "        F.max(\"humidity\").alias(\"max_humidity\"),\n",
    "        F.avg(\"humidity\").alias(\"avg_humidity\"),\n",
    "        F.min(\"temperature\").alias(\"min_temperature\"),\n",
    "        F.max(\"temperature\").alias(\"max_temperature\"),\n",
    "        F.avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "        F.min(\"pressure\").alias(\"min_pressure\"),\n",
    "        F.max(\"pressure\").alias(\"max_pressure\"),\n",
    "        F.avg(\"pressure\").alias(\"avg_pressure\")\n",
    "    ) \\\n",
    "    .withColumn(\"window_start\", col(\"window.start\")) \\\n",
    "    .withColumn(\"window_end\", col(\"window.end\")) \\\n",
    "    .drop(\"window\")\n",
    "\n",
    "aggregation_df = aggregation_df.select(\n",
    "    \"latitude\", \"longitude\", \"window_start\", \"window_end\",\n",
    "    \"min_humidity\", \"max_humidity\", \"avg_humidity\",\n",
    "    \"min_temperature\", \"max_temperature\", \"avg_temperature\",\n",
    "    \"min_pressure\", \"max_pressure\", \"avg_pressure\"\n",
    ")\n",
    "\n",
    "writer = aggregation_df.writeStream \\\n",
    "    .queryName(\"weather_sensor_summary_stream\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .toTable(f\"{gold_catalog_name}.{gold_schema_name}.{gold_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f29960f-3464-47ea-b488-71830c31d70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for q in spark.streams.active:\n",
    "#     if q.name == \"weather_sensor_summary_stream\":\n",
    "#         q.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_weather_data_streaming_pipeline_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
