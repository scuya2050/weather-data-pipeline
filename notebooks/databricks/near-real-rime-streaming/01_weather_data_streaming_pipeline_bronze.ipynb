{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd1fdea8-0c34-42cb-aec6-b0b4126d4521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddb00bd-dfe2-48d3-bff9-8e5ab29b97a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import schema_of_json, current_timestamp, input_file_name, lit, col, to_json, from_json, to_timestamp, to_utc_timestamp, md5, concat_ws\n",
    "from pyspark.sql.types import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c873b79a-ba79-48dc-8fab-33ce222045d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b31763ef-f366-425e-982b-54c98fa1a92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"weather\"\n",
    "schema_name = \"01_bronze\"\n",
    "table_name = \"weather_sensor_measurements_raw\"\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.checkpoints\")\n",
    "checkpoint_dir = f\"/Volumes/{catalog_name}/{schema_name}/checkpoints/{table_name}\"\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.bad_records\")\n",
    "bad_records_dir = f\"/Volumes/{catalog_name}/{schema_name}/bad_records/{table_name}\"\n",
    "\n",
    "s3_path = \"s3://databricks-aws-sebastiancuya-bucket/weather-sensor-data\"\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"device\", StringType(), True),\n",
    "    StructField(\"location\", StructType([\n",
    "        StructField(\"latitude\", DoubleType(), True),\n",
    "        StructField(\"longitude\", DoubleType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"request_timestamp\", TimestampType(), True),\n",
    "    StructField(\"measurement\", StructType([\n",
    "        StructField(\"humidity\", DoubleType(), True),\n",
    "        StructField(\"temperature\", DoubleType(), True),\n",
    "        StructField(\"pressure\", DoubleType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "307cff58-e7b2-455c-8dbf-e2a45e933084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .option(\"badRecordsPath\", bad_records_dir) \\\n",
    "    .schema(schema) \\\n",
    "    .load(s3_path)\n",
    "\n",
    "streaming_df = streaming_df \\\n",
    "    .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"file_path\", col(\"_metadata.file_path\")) \\\n",
    "    .withColumn(\"uuid\", md5(concat_ws(\"_\", col(\"device\"), col(\"measurement.timestamp\").cast(\"string\"))))\n",
    "\n",
    "streaming_df = streaming_df.selectExpr(\n",
    "    \"uuid as uuid\",\n",
    "    \"device as device\",\n",
    "    \"location.latitude as latitude\",\n",
    "    \"location.longitude as longitude\",\n",
    "    \"request_timestamp as request_timestamp\",\n",
    "    \"measurement.humidity as humidity\",\n",
    "    \"measurement.temperature as temperature\",\n",
    "    \"measurement.pressure as pressure\",\n",
    "    \"measurement.timestamp as measurement_timestamp\",\n",
    "    \"timestamp as ingestion_timestamp\",\n",
    "    \"file_path as file_path\"\n",
    ")\n",
    "\n",
    "query = streaming_df.writeStream \\\n",
    "    .queryName(\"weather_sensor_ingestion_stream\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .toTable(f\"{catalog_name}.{schema_name}.{table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649c6f36-f647-4821-8566-5b15a1e6e04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for q in spark.streams.active:\n",
    "#     if q.name == \"weather_sensor_ingestion_stream\":\n",
    "#         q.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7182025918984349,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_weather_data_streaming_pipeline_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
