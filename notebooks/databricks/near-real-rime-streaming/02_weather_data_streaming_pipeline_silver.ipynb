{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72b5fe36-e8e6-478f-97ee-34a08dbd1661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a24402d-232b-42b8-b91e-bc82ac17bb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import schema_of_json, current_timestamp, input_file_name, lit, col, to_json, from_json, to_timestamp, to_utc_timestamp\n",
    "from pyspark.sql.types import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6577af6-af4b-48de-96d7-bc42dcd3fd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f75074-2f56-4fb6-ac8a-04c2acd68a76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_catalog_name = \"weather\"\n",
    "bronze_schema_name = \"01_bronze\"\n",
    "bronze_table_name = \"weather_sensor_measurements_raw\"\n",
    "\n",
    "silver_catalog_name = \"weather\"\n",
    "silver_schema_name = \"02_silver\"\n",
    "silver_table_name = \"weather_sensor_measurements\"\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {silver_catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {silver_catalog_name}.{silver_schema_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {silver_catalog_name}.{silver_schema_name}.checkpoints\")\n",
    "checkpoint_dir = f\"/Volumes/{silver_catalog_name}/{silver_schema_name}/checkpoints/{silver_table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca4f651-a650-4850-ab2b-fd2d7a9b782c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_silver_table_sql_statement = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {silver_catalog_name}.{silver_schema_name}.{silver_table_name} (\n",
    "        uuid STRING PRIMARY KEY,\n",
    "        device STRING,\n",
    "        latitude DOUBLE,\n",
    "        longitude DOUBLE,\n",
    "        request_timestamp TIMESTAMP,\n",
    "        humidity DOUBLE,\n",
    "        temperature DOUBLE,\n",
    "        pressure DOUBLE,\n",
    "        measurement_timestamp TIMESTAMP,\n",
    "        ingestion_timestamp TIMESTAMP,\n",
    "        processing_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (device);\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_silver_table_sql_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc74912b-ad6f-4b31-aede-1d9bb8fbf13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"delta\") \\\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\") \\\n",
    "    .table(f\"{bronze_catalog_name}.{bronze_schema_name}.{bronze_table_name}\")\n",
    "\n",
    "processing_df = streaming_df \\\n",
    "    .filter(col(\"measurement_timestamp\").cast(\"long\") != 0) \\\n",
    "    .filter(col(\"pressure\").isNotNull()) \\\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp()) \\\n",
    "    .dropDuplicates(['uuid'])\n",
    "    \n",
    "\n",
    "column_names = [\"uuid\"] + [\n",
    "  line.split()[0] \n",
    "  for line in create_silver_table_sql_statement.splitlines()[1:] \n",
    "  if (\n",
    "    len(line.split()) == 2 \n",
    "    and re.match(r'([A-Z]*).*', line.split()[1].strip()).group(1) in [\n",
    "      'STRING', 'FLOAT', 'DOUBLE', 'BIGINT', 'BOOLEAN', 'TIMESTAMP', 'DECIMAL']\n",
    "    )\n",
    "  ]\n",
    "\n",
    "filtered_processing_df = processing_df.select(*column_names)\n",
    "\n",
    "# Function to upsert microBatchOutputDF into Delta table using merge\n",
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "  microBatchOutputDF.createOrReplaceTempView(\"streaming_updates\")\n",
    "\n",
    "  # In Databricks Runtime 10.5 and below, you must use the following:\n",
    "  # microBatchOutputDF._jdf.sparkSession().sql(\"\"\"\n",
    "  microBatchOutputDF.sparkSession.sql(f\"\"\"\n",
    "    MERGE INTO {silver_catalog_name}.{silver_schema_name}.{silver_table_name} t\n",
    "    USING streaming_updates s\n",
    "    ON \n",
    "        s.uuid = t.uuid\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "  \"\"\")\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta table\n",
    "writer = filtered_processing_df.writeStream \\\n",
    "    .queryName(\"weather_sensor_processing_stream\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\")\\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483decfd-44c8-4790-bb85-c8fa54e264e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for q in spark.streams.active:\n",
    "#     if q.name == \"weather_sensor_processing_stream\":\n",
    "#         q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479058be-c7c6-4e9d-8114-c7ae640244c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "#     OPTIMIZE {silver_catalog_name}.{silver_schema_name}.{silver_table_name}\n",
    "#     ZORDER BY (measurement_timestamp);\n",
    "#     \"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c87d7199-aae6-4c61-a06a-c7b63a3bea29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_weather_data_streaming_pipeline_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
