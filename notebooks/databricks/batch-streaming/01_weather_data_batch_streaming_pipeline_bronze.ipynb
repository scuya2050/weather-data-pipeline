{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1fdea8-0c34-42cb-aec6-b0b4126d4521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddb00bd-dfe2-48d3-bff9-8e5ab29b97a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import schema_of_json, current_timestamp, input_file_name, lit, col, to_json, from_json, to_timestamp, to_utc_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, LongType, BooleanType\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c873b79a-ba79-48dc-8fab-33ce222045d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791f9a84-8a14-45e7-b123-d83c2152ad9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def list_all_files(path):\n",
    "    file_paths = []\n",
    "\n",
    "    for item in dbutils.fs.ls(path):\n",
    "        if item.isDir():\n",
    "            file_paths = file_paths + list_all_files(item.path)\n",
    "        else:\n",
    "            file_paths.append(item.path)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def list_first_file(path):\n",
    "    file_path = None\n",
    "\n",
    "    for item in dbutils.fs.ls(path):\n",
    "        if item.isDir():\n",
    "            return list_first_file(item.path)\n",
    "        else:\n",
    "            file_path = item.path\n",
    "            return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b31763ef-f366-425e-982b-54c98fa1a92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"weather\"\n",
    "schema_name = \"bronze\"\n",
    "table_name = \"weather_measurements_raw\"\n",
    "\n",
    "volume_name = \"checkpoints\"\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.{volume_name}\")\n",
    "checkpoint_dir = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/{table_name}\"\n",
    "\n",
    "s3_path = \"s3a://databricks-aws-sebastiancuya-bucket/weather-data\"\n",
    "example_file_path = list_first_file(s3_path)\n",
    "\n",
    "df = spark.read.json(example_file_path)\n",
    "inferred_schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "307cff58-e7b2-455c-8dbf-e2a45e933084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(inferred_schema) \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(s3_path)\n",
    "\n",
    "streaming_df = streaming_df \\\n",
    "    .withColumn(\"timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "\n",
    "streaming_df = streaming_df.selectExpr(\n",
    "    \"location.name as district\",\n",
    "    \"location.region as region\",\n",
    "    \"location.country as country\",\n",
    "    \"location.lat as lat\",\n",
    "    \"location.lon as lon\",\n",
    "    \"location.tz_id as tz_id\",\n",
    "    \"location.localtime_epoch as localtime_epoch\",\n",
    "    \"location.localtime as localtime\",\n",
    "    \"current.last_updated_epoch as last_updated_epoch\",\n",
    "    \"current.last_updated as last_updated\",\n",
    "    \"current.temp_c as temp_c\",\n",
    "    \"current.temp_f as temp_f\",\n",
    "    \"current.is_day as is_day\",\n",
    "    \"current.condition.text as condition_text\",\n",
    "    \"current.condition.icon as condition_icon\",\n",
    "    \"current.condition.code as condition_code\",\n",
    "    \"current.wind_mph as wind_mph\",\n",
    "    \"current.wind_kph as wind_kph\",\n",
    "    \"current.wind_degree as wind_degree\",\n",
    "    \"current.wind_dir as wind_dir\",\n",
    "    \"current.pressure_mb as pressure_mb\",\n",
    "    \"current.pressure_in as pressure_in\",\n",
    "    \"current.precip_mm as precip_mm\",\n",
    "    \"current.precip_in as precip_in\",\n",
    "    \"current.humidity as humidity\",\n",
    "    \"current.cloud as cloud\",\n",
    "    \"current.feelslike_c as feelslike_c\",\n",
    "    \"current.feelslike_f as feelslike_f\",\n",
    "    \"current.windchill_c as windchill_c\",\n",
    "    \"current.windchill_f as windchill_f\",\n",
    "    \"current.heatindex_c as heatindex_c\",\n",
    "    \"current.heatindex_f as heatindex_f\",\n",
    "    \"current.dewpoint_c as dewpoint_c\",\n",
    "    \"current.dewpoint_f as dewpoint_f\",\n",
    "    \"current.vis_km as vis_km\",\n",
    "    \"current.vis_miles as vis_miles\",\n",
    "    \"current.uv as uv\",\n",
    "    \"current.gust_mph as gust_mph\",\n",
    "    \"current.gust_kph as gust_kph\",\n",
    "    \"current.air_quality.`co` as air_quality_co\",\n",
    "    \"current.air_quality.`no2` as air_quality_no2\",\n",
    "    \"current.air_quality.`o3` as air_quality_o3\",\n",
    "    \"current.air_quality.`so2` as air_quality_so2\",\n",
    "    \"current.air_quality.`pm2_5` as air_quality_pm2_5\",\n",
    "    \"current.air_quality.`pm10` as air_quality_pm10\",\n",
    "    \"current.air_quality.`us-epa-index` as air_quality_us_epa_index\",\n",
    "    \"current.air_quality.`gb-defra-index` as air_quality_gb_defra_index\",\n",
    "    \"timestamp as ingestion_timestamp\",\n",
    "    \"file_path as file_path\"\n",
    ")\n",
    "\n",
    "writer = streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .trigger(once=True) \\\n",
    "    .toTable(f\"{catalog_name}.{schema_name}.{table_name}\")\n",
    "\n",
    "writer.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7182025918984349,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_weather_data_batch_streaming_pipeline_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
