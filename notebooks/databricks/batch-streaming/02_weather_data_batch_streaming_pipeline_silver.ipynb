{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b5fe36-e8e6-478f-97ee-34a08dbd1661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a24402d-232b-42b8-b91e-bc82ac17bb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import schema_of_json, current_timestamp, input_file_name, lit, col, to_json, from_json, to_timestamp, to_utc_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, LongType, BooleanType\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6577af6-af4b-48de-96d7-bc42dcd3fd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f75074-2f56-4fb6-ac8a-04c2acd68a76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_catalog_name = \"weather\"\n",
    "bronze_schema_name = \"bronze\"\n",
    "bronze_table_name = \"weather_measurements_raw\"\n",
    "\n",
    "silver_catalog_name = \"weather\"\n",
    "silver_schema_name = \"silver\"\n",
    "silver_table_name = \"weather_measurements\"\n",
    "\n",
    "silver_volume_name = \"checkpoints\"\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {silver_catalog_name}.{silver_schema_name}.{silver_volume_name}\")\n",
    "checkpoint_dir = f\"/Volumes/{silver_catalog_name}/{silver_schema_name}/{silver_volume_name}/{silver_table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca4f651-a650-4850-ab2b-fd2d7a9b782c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_silver_table_sql_statement = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {silver_catalog_name}.{silver_schema_name}.{silver_table_name} (\n",
    "        country STRING,\n",
    "        region STRING,\n",
    "        district STRING,\n",
    "        lat DECIMAL(10,6),\n",
    "        lon DECIMAL(10,6),\n",
    "        tz_id STRING,\n",
    "        api_call_timestamp TIMESTAMP,\n",
    "        last_updated_timestamp TIMESTAMP,\n",
    "        temp_c DECIMAL(10,2),\n",
    "        is_day BOOLEAN,\n",
    "        weather_condition STRING,\n",
    "        wind_mph DECIMAL(10,2),\n",
    "        wind_degree DECIMAL(10,2),\n",
    "        pressure_mb DECIMAL(10,2),\n",
    "        humidity DECIMAL(10,2),\n",
    "        cloud DECIMAL(10,2),\n",
    "        feelslike_c DECIMAL(10,2),\n",
    "        windchill_c DECIMAL(10,2),\n",
    "        heatindex_c DECIMAL(10,2),\n",
    "        dewpoint_c DECIMAL(10,2),\n",
    "        vis_km DECIMAL(10,2),\n",
    "        gust_mph DECIMAL(10,2),\n",
    "        air_quality_co DECIMAL(10,2),\n",
    "        air_quality_no2 DECIMAL(10,2),\n",
    "        air_quality_o3 DECIMAL(10,2),\n",
    "        air_quality_so2 DECIMAL(10,2),\n",
    "        air_quality_pm2_5 DECIMAL(10,2),\n",
    "        air_quality_pm10 DECIMAL(10,2),\n",
    "        air_quality_us_epa_index BIGINT,\n",
    "        air_quality_gb_defra_index BIGINT,\n",
    "        ingestion_timestamp TIMESTAMP,\n",
    "        processing_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (country);\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_silver_table_sql_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc74912b-ad6f-4b31-aede-1d9bb8fbf13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .table(f\"{bronze_catalog_name}.{bronze_schema_name}.{bronze_table_name}\")\n",
    "\n",
    "processing_df = streaming_df \\\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"api_call_timestamp\", to_utc_timestamp(to_timestamp(\"localtime\", \"yyyy-MM-dd HH:mm\"), streaming_df.tz_id)) \\\n",
    "    .withColumn(\"last_updated_timestamp\", to_utc_timestamp(to_timestamp(\"last_updated\", \"yyyy-MM-dd HH:mm\"), streaming_df.tz_id))\\\n",
    "    .withColumn(\"is_day\", col(\"is_day\").cast(\"boolean\")) \\\n",
    "    .withColumn(\"humidity\", col(\"humidity\").cast(\"float\")) \\\n",
    "    .withColumn(\"cloud\", col(\"cloud\").cast(\"float\")) \\\n",
    "    .withColumnRenamed(\"condition_text\", \"weather_condition\") \\\n",
    "    .dropDuplicates(['country', 'region', 'district', 'last_updated_timestamp'])\n",
    "    \n",
    "\n",
    "column_names = [\n",
    "  line.split()[0] \n",
    "  for line in create_silver_table_sql_statement.splitlines()[1:] \n",
    "  if (\n",
    "    len(line.split()) == 2 \n",
    "    and re.match(r'([A-Z]*).*', line.split()[1].strip()).group(1) in [\n",
    "      'STRING', 'FLOAT', 'DOUBLE', 'BIGINT', 'BOOLEAN', 'TIMESTAMP', 'DECIMAL']\n",
    "    )\n",
    "  ]\n",
    "\n",
    "filtered_processing_df = processing_df.select(*column_names)\n",
    "\n",
    "# Function to upsert microBatchOutputDF into Delta table using merge\n",
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "  microBatchOutputDF.createOrReplaceTempView(\"streaming_updates\")\n",
    "\n",
    "  # In Databricks Runtime 10.5 and below, you must use the following:\n",
    "  # microBatchOutputDF._jdf.sparkSession().sql(\"\"\"\n",
    "  microBatchOutputDF.sparkSession.sql(f\"\"\"\n",
    "    MERGE INTO {silver_catalog_name}.{silver_schema_name}.{silver_table_name} t\n",
    "    USING streaming_updates s\n",
    "    ON \n",
    "        s.country = t.country\n",
    "        AND s.region = t.region\n",
    "        AND s.district = t.district\n",
    "        AND s.last_updated_timestamp = t.last_updated_timestamp\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "  \"\"\")\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta table\n",
    "writer = filtered_processing_df.writeStream \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\")\\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "writer.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479058be-c7c6-4e9d-8114-c7ae640244c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'OPTIMIZE {silver_catalog_name}.{silver_schema_name}.{silver_table_name} ZORDER BY (last_updated_timestamp)')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_weather_data_batch_streaming_pipeline_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
